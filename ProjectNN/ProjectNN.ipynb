{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model,load_model\n",
    "from keras.preprocessing import *\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import batch_normalization\n",
    "from keras.models import *\n",
    "from keras.layers import Input, Conv2D, SeparableConv2D, Add, Dense, BatchNormalization, ReLU, MaxPool2D, GlobalAvgPool2D, Concatenate, Average,Maximum\n",
    "from keras.layers import Input\n",
    "from keras.utils.data_utils import get_file"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-24T23:13:48.784141Z",
     "iopub.execute_input": "2022-12-24T23:13:48.784927Z",
     "iopub.status.idle": "2022-12-24T23:13:48.795369Z",
     "shell.execute_reply.started": "2022-12-24T23:13:48.784881Z",
     "shell.execute_reply": "2022-12-24T23:13:48.793989Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "TRAIN_DIR = '/kaggle/input/nn23-sports-image-classification/Train'\n",
    "TEST_DIR = '/kaggle/input/nn23-sports-image-classification/Test'\n",
    "IMG_SIZE = 224\n",
    "MODEL_NAME = 'sports-image-classification'"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-24T23:13:48.797398Z",
     "iopub.execute_input": "2022-12-24T23:13:48.798210Z",
     "iopub.status.idle": "2022-12-24T23:13:48.814327Z",
     "shell.execute_reply.started": "2022-12-24T23:13:48.798168Z",
     "shell.execute_reply": "2022-12-24T23:13:48.813160Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#     \"\"\" Create an one-hot encoded vector from image name \"\"\"\n",
    "def create_label(image_name):\n",
    "    word_label = image_name.split('_')\n",
    "    if word_label[0] == 'Basketball':\n",
    "        return np.array([1,0, 0, 0, 0,0])\n",
    "    elif word_label[0] == 'Football':\n",
    "        return np.array([0,1, 0, 0, 0,0])\n",
    "    elif word_label[0] == 'Rowing':\n",
    "        return np.array([0,0, 1, 0, 0,0])\n",
    "    elif word_label[0] == 'Swimming':\n",
    "        return np.array([0,0, 0, 1, 0,0])\n",
    "    elif word_label[0] == 'Tennis':\n",
    "        return np.array([0,0, 0, 0, 1,0])\n",
    "    elif word_label[0] == 'Yoga':\n",
    "        return np.array([0,0, 0, 0, 0,1])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-24T23:13:48.816809Z",
     "iopub.execute_input": "2022-12-24T23:13:48.817176Z",
     "iopub.status.idle": "2022-12-24T23:13:48.838086Z",
     "shell.execute_reply.started": "2022-12-24T23:13:48.817141Z",
     "shell.execute_reply": "2022-12-24T23:13:48.831854Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#preprocessing\n",
    "def zoom_at(img, zoom=1, angle=0, coord=None):\n",
    "    cy, cx = [ i/2 for i in img.shape[:-1] ] if coord is None else coord[::-1]\n",
    "    rot_mat = cv2.getRotationMatrix2D((cx,cy), angle, zoom)\n",
    "    result = cv2.warpAffine(img, rot_mat, img.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
    "    return result\n",
    "\n",
    "def Rotation(img,deg):\n",
    "    (h, w) = img.shape[:2]\n",
    "    (cX, cY) = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D((cX, cY), deg, 1.0)\n",
    "    rotated = cv2.warpAffine(img, M, (w, h))\n",
    "    return rotated\n",
    "\n",
    "def create_train_data():\n",
    "    training_data = []\n",
    "    for img in tqdm(os.listdir(TRAIN_DIR)):\n",
    "        path = os.path.join(TRAIN_DIR, img)\n",
    "        img_data = cv2.imread(path)\n",
    "        img_data = cv2.resize(img_data, (IMG_SIZE, IMG_SIZE))\n",
    "        label = create_label(img)\n",
    "        \n",
    "        #Augmentation\n",
    "        flip1=cv2.flip(img_data,1)\n",
    "        flip2=cv2.flip(img_data,0)\n",
    "        flip3=cv2.flip(img_data,-1)\n",
    "        rotated = Rotation(img_data,45)\n",
    "        flip4=cv2.flip(flip3,1)\n",
    "        flip5=cv2.flip(rotated,0)\n",
    "        flip6=cv2.flip(flip5,-1)\n",
    "        rotated1 = Rotation(flip6, 90)\n",
    "        zoom=cv2.flip(rotated1,-1)\n",
    "        zoom1=cv2.flip(flip6,1)\n",
    "        \n",
    "        training_data.append([np.array(img_data), label])\n",
    "        training_data.append([np.array(flip1), label])\n",
    "        training_data.append([np.array(flip2), label])\n",
    "        training_data.append([np.array(flip3), label]) \n",
    "        training_data.append([np.array(rotated), label])\n",
    "        training_data.append([np.array(flip4), label])\n",
    "        training_data.append([np.array(flip5), label])\n",
    "        training_data.append([np.array(flip6), label])\n",
    "        training_data.append([np.array(rotated1), label])\n",
    "        training_data.append([np.array(zoom), label])\n",
    "        training_data.append([np.array(zoom1), label])\n",
    "   \n",
    "    shuffle(training_data)\n",
    "    np.save('train_data.npy', training_data)\n",
    "    return training_data"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-24T23:13:48.839995Z",
     "iopub.execute_input": "2022-12-24T23:13:48.841067Z",
     "iopub.status.idle": "2022-12-24T23:13:48.870459Z",
     "shell.execute_reply.started": "2022-12-24T23:13:48.841020Z",
     "shell.execute_reply": "2022-12-24T23:13:48.869665Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if (os.path.exists('train_data.npy')): # If you have already created the dataset:\n",
    "    train =np.load('/kaggle/working/train_data.npy',allow_pickle=True)\n",
    "else:#If dataset is not created:\n",
    "    train = create_train_data()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-24T23:13:48.876881Z",
     "iopub.execute_input": "2022-12-24T23:13:48.879568Z",
     "iopub.status.idle": "2022-12-24T23:14:00.682664Z",
     "shell.execute_reply.started": "2022-12-24T23:13:48.879518Z",
     "shell.execute_reply": "2022-12-24T23:14:00.681599Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_train = np.array([i[0] for i in train]).reshape(-1, IMG_SIZE, IMG_SIZE, 3) \n",
    "Y_train = np.asarray([i[1] for i in train])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-24T23:14:00.684186Z",
     "iopub.execute_input": "2022-12-24T23:14:00.684565Z",
     "iopub.status.idle": "2022-12-24T23:14:04.601492Z",
     "shell.execute_reply.started": "2022-12-24T23:14:00.684526Z",
     "shell.execute_reply": "2022-12-24T23:14:04.600380Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# #VGG16 LAST MODEL\n",
    "# WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "# weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',WEIGHTS_PATH_NO_TOP)\n",
    "\n",
    "# from tensorflow.keras.layers import AveragePooling2D, Dropout, Flatten, Dense#vGG16 with keras\n",
    "# model = Sequential()\n",
    "# # input_layer=input_data(shape=(224,224,3))\n",
    "# input_layer=Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "# conv_l1=Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\")(input_layer)\n",
    "# conv_l2=Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\")(conv_l1)\n",
    "# pool1=MaxPooling2D(pool_size=(2,2),strides=(2,2))(conv_l2)\n",
    "# conv_l3=Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool1)\n",
    "# conv_l4=Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv_l3)\n",
    "# pool2=MaxPooling2D(pool_size=(2,2),strides=(2,2))(conv_l4)\n",
    "# conv_l4=Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool2)\n",
    "# conv_l5=Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv_l4)\n",
    "# conv_l6=Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv_l5)\n",
    "# pool3=MaxPooling2D(pool_size=(2,2),strides=(2,2))(conv_l6)\n",
    "# conv_l6=Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool3)\n",
    "# conv_l7=Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv_l6)\n",
    "# conv_l8=Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv_l7)\n",
    "# pool4=MaxPooling2D(pool_size=(2,2),strides=(2,2))(conv_l8)\n",
    "# conv_l8=Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(pool4)\n",
    "# conv_l9=Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv_l8)\n",
    "# conv_l10=Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(conv_l9)\n",
    "# pool5=MaxPooling2D(pool_size=(2,2),strides=(2,2))(conv_l10)\n",
    "# model=Model(inputs=input_layer,outputs=pool5)\n",
    "# model.load_weights(weights_path)\n",
    "# out=model.output\n",
    "# model.trainable=False\n",
    "# LAYER=tf.keras.layers.GlobalAveragePooling2D()\n",
    "# out=LAYER(out)\n",
    "# out=Dense(3072,activation=\"relu\")(out)\n",
    "# out=Dense(6,activation=\"softmax\")(out)\n",
    "# Vgg16=Model(inputs=model.input,outputs=out)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-24T23:14:04.608335Z",
     "iopub.execute_input": "2022-12-24T23:14:04.608653Z",
     "iopub.status.idle": "2022-12-24T23:14:04.615775Z",
     "shell.execute_reply.started": "2022-12-24T23:14:04.608625Z",
     "shell.execute_reply": "2022-12-24T23:14:04.614458Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Alex Net\n",
    "AlexNet = Sequential()\n",
    "\n",
    "# 1st Convolutional Layer\n",
    "#  image_shape=(IMG_SIZE,IMG_SIZE,3)\n",
    "AlexNet.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11), strides=(4,4), padding='valid'))\n",
    "AlexNet.add(Activation('relu'))\n",
    "# Max Pooling\n",
    "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n",
    "AlexNet.add(Activation('relu'))\n",
    "# Max Pooling\n",
    "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "# 3rd Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "# 4th Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "# 5th Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "AlexNet.add(Activation('relu'))\n",
    "# Max Pooling\n",
    "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "#model.load_wights(wieghts_path)\n",
    "#model.trainable=false\n",
    "\n",
    "# Passing it to a Fully Connected layer\n",
    "AlexNet.add(Flatten())\n",
    "# 1st Fully Connected Layer\n",
    "AlexNet.add(Dense(4096, input_shape=(224,224,)))\n",
    "AlexNet.add(Activation('relu'))\n",
    "# Add Dropout to prevent overfitting\n",
    "AlexNet.add(Dropout(0.4))\n",
    "\n",
    "# 2nd Fully Connected Layer\n",
    "AlexNet.add(Dense(4096))\n",
    "AlexNet.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "AlexNet.add(Dropout(0.4))\n",
    "\n",
    "# 3rd Fully Connected Layer\n",
    "AlexNet.add(Dense(1000))\n",
    "AlexNet.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "AlexNet.add(Dropout(0.4))\n",
    "\n",
    "# Output Layer\n",
    "AlexNet.add(Dense(6))\n",
    "AlexNet.add(Activation('softmax'))\n",
    "\n",
    "AlexNet.summary()\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-24T23:14:04.617946Z",
     "iopub.execute_input": "2022-12-24T23:14:04.618311Z",
     "iopub.status.idle": "2022-12-24T23:14:05.064574Z",
     "shell.execute_reply.started": "2022-12-24T23:14:04.618274Z",
     "shell.execute_reply": "2022-12-24T23:14:05.063415Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_13 (Conv2D)           (None, 54, 54, 96)        34944     \n_________________________________________________________________\nactivation (Activation)      (None, 54, 54, 96)        0         \n_________________________________________________________________\nmax_pooling2d_5 (MaxPooling2 (None, 27, 27, 96)        0         \n_________________________________________________________________\nconv2d_14 (Conv2D)           (None, 17, 17, 256)       2973952   \n_________________________________________________________________\nactivation_1 (Activation)    (None, 17, 17, 256)       0         \n_________________________________________________________________\nmax_pooling2d_6 (MaxPooling2 (None, 8, 8, 256)         0         \n_________________________________________________________________\nconv2d_15 (Conv2D)           (None, 6, 6, 384)         885120    \n_________________________________________________________________\nactivation_2 (Activation)    (None, 6, 6, 384)         0         \n_________________________________________________________________\nconv2d_16 (Conv2D)           (None, 4, 4, 384)         1327488   \n_________________________________________________________________\nactivation_3 (Activation)    (None, 4, 4, 384)         0         \n_________________________________________________________________\nconv2d_17 (Conv2D)           (None, 2, 2, 256)         884992    \n_________________________________________________________________\nactivation_4 (Activation)    (None, 2, 2, 256)         0         \n_________________________________________________________________\nmax_pooling2d_7 (MaxPooling2 (None, 1, 1, 256)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 256)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 4096)              1052672   \n_________________________________________________________________\nactivation_5 (Activation)    (None, 4096)              0         \n_________________________________________________________________\ndropout (Dropout)            (None, 4096)              0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 4096)              16781312  \n_________________________________________________________________\nactivation_6 (Activation)    (None, 4096)              0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 4096)              0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 1000)              4097000   \n_________________________________________________________________\nactivation_7 (Activation)    (None, 1000)              0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 1000)              0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 6)                 6006      \n_________________________________________________________________\nactivation_8 (Activation)    (None, 6)                 0         \n=================================================================\nTotal params: 28,043,486\nTrainable params: 28,043,486\nNon-trainable params: 0\n_________________________________________________________________\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "optimization = tf.keras.optimizers.SGD(lr=.0001,decay=1e-6 ,momentum=0.9,nesterov=True,name=\"SGD\")\n",
    "\n",
    "AlexNet.compile(optimizer=optimization, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "AlexNet.fit(X_train, Y_train,validation_split=0.2 ,batch_size=1, epochs=13, verbose=1)\n",
    "AlexNet.save('Alex_net.tfl')\n",
    "\n",
    "# Vgg16.compile(optimizer=optimization, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Vgg16.fit(X_train, Y_train,validation_split=0.2 ,batch_size=1, epochs=7, verbose=1)\n",
    "# Vgg16.save('Vgg16.tfl')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-24T23:14:05.066856Z",
     "iopub.execute_input": "2022-12-24T23:14:05.067474Z",
     "iopub.status.idle": "2022-12-24T23:14:19.748781Z",
     "shell.execute_reply.started": "2022-12-24T23:14:05.067436Z",
     "shell.execute_reply": "2022-12-24T23:14:19.746605Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": "Train on 14792 samples, validate on 3699 samples\nEpoch 1/25\n 1551/14792 [==>...........................] - ETA: 1:50 - loss: 1.8146 - acc: 0.2631",
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_10641/141312287.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mAlexNet\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0moptimization\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'categorical_crossentropy'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'accuracy'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mAlexNet\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mY_train\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mvalidation_split\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.2\u001B[0m \u001B[0;34m,\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m25\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0mAlexNet\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Alex_net.tfl'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training_v1.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001B[0m\n\u001B[1;32m    794\u001B[0m         \u001B[0mmax_queue_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmax_queue_size\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    795\u001B[0m         \u001B[0mworkers\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mworkers\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 796\u001B[0;31m         use_multiprocessing=use_multiprocessing)\n\u001B[0m\u001B[1;32m    797\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    798\u001B[0m   def evaluate(self,\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training_arrays_v1.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001B[0m\n\u001B[1;32m    655\u001B[0m         \u001B[0mvalidation_steps\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mvalidation_steps\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    656\u001B[0m         \u001B[0mvalidation_freq\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mvalidation_freq\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 657\u001B[0;31m         steps_name='steps_per_epoch')\n\u001B[0m\u001B[1;32m    658\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    659\u001B[0m   def evaluate(self,\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training_arrays_v1.py\u001B[0m in \u001B[0;36mmodel_iteration\u001B[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001B[0m\n\u001B[1;32m    374\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    375\u001B[0m         \u001B[0;31m# Get outputs.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 376\u001B[0;31m         \u001B[0mbatch_outs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mins_batch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    377\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch_outs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    378\u001B[0m           \u001B[0mbatch_outs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mbatch_outs\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/keras/backend.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m   4030\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4031\u001B[0m     fetched = self._callable_fn(*array_vals,\n\u001B[0;32m-> 4032\u001B[0;31m                                 run_metadata=self.run_metadata)\n\u001B[0m\u001B[1;32m   4033\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_fetch_callbacks\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfetched\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fetches\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4034\u001B[0m     output_structure = tf.nest.pack_sequence_as(\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1478\u001B[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001B[1;32m   1479\u001B[0m                                                \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_handle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1480\u001B[0;31m                                                run_metadata_ptr)\n\u001B[0m\u001B[1;32m   1481\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mrun_metadata\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1482\u001B[0m           \u001B[0mproto_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf_session\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTF_GetBuffer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrun_metadata_ptr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "idx =[]\n",
    "img_name = []\n",
    "for test_img in tqdm(os.listdir(TEST_DIR)):\n",
    "    path = os.path.join(TEST_DIR, test_img)\n",
    "    img_data = cv2.imread(path)\n",
    "    img_data = cv2.resize(img_data, (IMG_SIZE, IMG_SIZE))\n",
    "    img_data = img_data.reshape(-1,IMG_SIZE, IMG_SIZE,3)\n",
    "\n",
    "    prediction = Vgg16.predict([img_data])[0]\n",
    "    idx.append(np.argmax(prediction))\n",
    "    img_name.append(test_img)\n",
    "out = pd.DataFrame()\n",
    "out[\"image_name\"]=img_name\n",
    "out[\"label\"]=idx\n",
    "out.to_csv('out.csv',index=False)\n",
    "out.head()\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-24T23:14:19.750035Z",
     "iopub.status.idle": "2022-12-24T23:14:19.751459Z",
     "shell.execute_reply.started": "2022-12-24T23:14:19.751124Z",
     "shell.execute_reply": "2022-12-24T23:14:19.751156Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # lab model\n",
    "# tf.compat.v1.reset_default_graph()\n",
    "# conv_input = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')\n",
    "# conv1 = conv_2d(conv_input, 32, 5, activation ='relu',padding='same')\n",
    "# pool1 = max_pool_2d(conv1, 5)\n",
    "\n",
    "# conv2 = conv_2d(pool1, 64, 5, activation='relu',padding='same')\n",
    "# pool2 = max_pool_2d(conv2, 5)\n",
    "\n",
    "# conv3 = conv_2d(pool2, 128, 5, activation='relu',padding='same')\n",
    "# pool3 = max_pool_2d(conv3, 5)\n",
    "\n",
    "# conv4 = conv_2d(pool3, 128, 5, activation='relu',padding='same')\n",
    "# pool4 = max_pool_2d(conv4, 5)\n",
    "\n",
    "# conv5 = conv_2d(pool4, 32, 5, activation='relu',padding='same')\n",
    "# pool5 = max_pool_2d(conv5, 5)\n",
    "\n",
    "# fully_layer = fully_connected(pool5, 512, activation='relu')\n",
    "# fully_layer1 = fully_connected(fully_layer, 512, activation='relu')\n",
    "\n",
    "# cnn_layers = fully_connected(fully_layer1, 6, activation='softmax')\n",
    "\n",
    "# cnn_layers = regression(cnn_layers, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n",
    "# model = tflearn.DNN(cnn_layers, tensorboard_dir='log', tensorboard_verbose=3)\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-24T23:14:19.752786Z",
     "iopub.status.idle": "2022-12-24T23:14:19.753682Z",
     "shell.execute_reply.started": "2022-12-24T23:14:19.753419Z",
     "shell.execute_reply": "2022-12-24T23:14:19.753444Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# #fares & Mohammes\n",
    "# # epochs = 15\n",
    "# # VGG model ver 16\n",
    "# tf.compat.v1.reset_default_graph()\n",
    "# conv_input = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')\n",
    "# conv1 = conv_2d(conv_input, 64, 3, activation='relu')\n",
    "# conv2 = conv_2d(conv1, 64, 3, activation='relu')\n",
    "# pool1 = max_pool_2d(conv2, 3)\n",
    "\n",
    "# conv3 = conv_2d(pool1, 128, 3, activation='relu')\n",
    "# conv4 = conv_2d(conv3, 128, 3, activation='relu')\n",
    "# pool2 = max_pool_2d(conv4, 3)\n",
    "\n",
    "# conv5 = conv_2d(pool2, 256, 3, activation='relu')\n",
    "# conv6 = conv_2d(conv5, 256, 3, activation='relu')\n",
    "# pool3 = max_pool_2d(conv6, 3)\n",
    "\n",
    "# conv7 = conv_2d(pool3, 512, 3, activation='relu')\n",
    "# conv8 = conv_2d(conv7, 512, 3, activation='relu')\n",
    "# conv9 = conv_2d(conv8, 512, 3, activation='relu')\n",
    "# pool4 = max_pool_2d(conv9, 3)\n",
    "\n",
    "# conv10 = conv_2d(pool4, 512, 3, activation='relu')\n",
    "# conv11 = conv_2d(conv10, 512, 3, activation='relu')\n",
    "# conv12 = conv_2d(conv11, 512, 3, activation='relu')\n",
    "# pool5 = max_pool_2d(conv12, 3)\n",
    "\n",
    "\n",
    "\n",
    "# fully_layer = fully_connected(pool4, 4096, activation='relu')\n",
    "# fully_layer = dropout(fully_layer, 0.5)\n",
    "# fully_layer1 = fully_connected(fully_layer, 4096, activation='relu')\n",
    "# fully_layer1 = dropout(fully_layer1, 0.5)\n",
    "# fully_layer2 = fully_connected(fully_layer1, 1000, activation='relu')\n",
    "# fully_layer2 = dropout(fully_layer2, 0.5)\n",
    "\n",
    "# cnn_layers = fully_connected(fully_layer2, 6, activation='softmax')\n",
    "\n",
    "# cnn_layers = regression(cnn_layers, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n",
    "# model = tflearn.DNN(cnn_layers, tensorboard_dir='log', tensorboard_verbose=3)\n",
    "# print (X_train.shape)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-24T23:14:19.755165Z",
     "iopub.status.idle": "2022-12-24T23:14:19.756059Z",
     "shell.execute_reply.started": "2022-12-24T23:14:19.755784Z",
     "shell.execute_reply": "2022-12-24T23:14:19.755828Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # # #Khaled & Mohammed\n",
    "# tf.compat.v1.reset_default_graph()\n",
    "# conv_input = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')\n",
    "# conv1 = conv_2d(conv_input, 32, 5, activation='relu')\n",
    "# pool1 = max_pool_2d(conv1, 2)\n",
    "\n",
    "# conv2 = conv_2d(pool1, 32, 5, activation='relu')\n",
    "# pool2 = max_pool_2d(conv2, 2)\n",
    "\n",
    "# conv3 = conv_2d(pool2, 64, 3, activation='relu')\n",
    "# pool3 = max_pool_2d(conv3, 3)\n",
    "# conv4 = conv_2d(pool3, 64, 3, activation='relu')\n",
    "\n",
    "\n",
    "# fully_layer = fully_connected(conv4, 256, activation='relu')\n",
    "# fully_layer1 = dropout(fully_layer, 0.2)\n",
    "\n",
    "# cnn_layers = fully_connected(fully_layer1, 6, activation='softmax')\n",
    "\n",
    "# cnn_layers = regression(cnn_layers, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n",
    "# model = tflearn.DNN(cnn_layers, tensorboard_dir='log', tensorboard_verbose=3)\n",
    "# print (X_train.shape)\n",
    "\n",
    "# # updated lab model\n",
    "# tf.compat.v1.reset_default_graph()\n",
    "# conv_input = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')\n",
    "\n",
    "# conv1 = conv_2d(conv_input, 32, 5, activation='relu')\n",
    "# pool1 = max_pool_2d(conv1, 5)\n",
    "\n",
    "# conv2 = conv_2d(pool1, 64, 5, activation='relu')\n",
    "# pool2 = max_pool_2d(conv2, 5)\n",
    "\n",
    "# conv3 = conv_2d(pool2, 128, 5, activation='relu')\n",
    "# pool3 = max_pool_2d(conv3, 5)\n",
    "\n",
    "# conv4 = conv_2d(pool3, 64, 5, activation='relu')\n",
    "# pool4 = max_pool_2d(conv4, 5)\n",
    "\n",
    "# conv5 = conv_2d(pool4, 32, 5, activation='relu')\n",
    "# pool5 = max_pool_2d(conv5, 5)\n",
    "\n",
    "# conv6 = conv_2d(pool5, 64, 5, activation='relu')\n",
    "# pool6 = max_pool_2d(conv6, 5)\n",
    "\n",
    "# conv7 = conv_2d(pool6, 128, 5, activation='relu')\n",
    "# pool7 = max_pool_2d(conv7, 5)\n",
    "\n",
    "# conv8 = conv_2d(pool7, 64, 5, activation='relu')\n",
    "# pool8 = max_pool_2d(conv8, 5)\n",
    "\n",
    "# conv9 = conv_2d(pool8, 32, 5, activation='relu')\n",
    "# pool9 = max_pool_2d(conv9, 5)\n",
    "\n",
    "# fully_layer = fully_connected(pool9, 1024, activation='relu')\n",
    "# fully_layer = dropout(fully_layer, 0.5)\n",
    "\n",
    "# cnn_layers = fully_connected(fully_layer, 6, activation='softmax')\n",
    "\n",
    "# cnn_layers = regression(cnn_layers, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n",
    "# model = tflearn.DNN(cnn_layers, tensorboard_dir='log', tensorboard_verbose=3)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-24T23:14:19.757609Z",
     "iopub.status.idle": "2022-12-24T23:14:19.758329Z",
     "shell.execute_reply.started": "2022-12-24T23:14:19.758054Z",
     "shell.execute_reply": "2022-12-24T23:14:19.758080Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# #AlexNet_model by using tflearn\n",
    "# tf.compat.v1.reset_default_graph()\n",
    "# Input_layer=conv_input = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')\n",
    "# Conv_layer1=conv_2d(Input_layer,96,11,activation='relu')\n",
    "# Max_pool_layer1=max_pool_2d(Conv_layer1, 3)\n",
    "# Conv_layer2=conv_2d(Max_pool_layer1,256,5,activation='relu')\n",
    "# Max_pool_layer2=max_pool_2d(Conv_layer2, 3)\n",
    "# Conv_layer3=conv_2d(Max_pool_layer2,384,3,activation='relu')\n",
    "# Conv_layer4=conv_2d(Conv_layer3,384,3,activation='relu')\n",
    "# Conv_layer5=conv_2d(Conv_layer4,256,3,activation='relu')\n",
    "# Max_pool_layer3=max_pool_2d(Conv_layer5, 3)\n",
    "\n",
    "# fully_conn1=fully_connected(Max_pool_layer3, 4096, activation='relu')\n",
    "# fully_conn1 = dropout(fully_conn1, 0.5)\n",
    "# fully_conn2=fully_connected(fully_conn1, 4096, activation='relu')\n",
    "# fully_conn2= dropout(fully_conn2, 0.5)\n",
    "\n",
    "# fully_conn2_softmax=fully_connected(fully_conn2, 6, activation='softmax')\n",
    "# CNN_model=regression(fully_conn2_softmax,optimizer='SGD', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n",
    "# model=tflearn.DNN(CNN_model, tensorboard_dir='log', tensorboard_verbose=3)\n",
    "# print(X_train.shape)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-24T23:14:19.759864Z",
     "iopub.status.idle": "2022-12-24T23:14:19.760746Z",
     "shell.execute_reply.started": "2022-12-24T23:14:19.760472Z",
     "shell.execute_reply": "2022-12-24T23:14:19.760499Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#ZFNet_Model\n",
    "# tf.compat.v1.reset_default_graph()\n",
    "# Input_layer=conv_input = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')\n",
    "# Conv_layer1=conv_2d(Input_layer,32,7,activation='relu')\n",
    "# Max_pool_layer1=max_pool_2d(Conv_layer1, 3)\n",
    "# Conv_layer2=conv_2d(Max_pool_layer1,52,5,activation='relu')\n",
    "# Max_pool_layer2=max_pool_2d(Conv_layer2, 3)\n",
    "# Conv_layer3=conv_2d(Max_pool_layer2,512,3,activation='relu')\n",
    "# Conv_layer4=conv_2d(Conv_layer3,1024,3,activation='relu')\n",
    "# Conv_layer5=conv_2d(Conv_layer4,512,3,activation='relu')\n",
    "# Max_pool_layer3=max_pool_2d(Conv_layer5, 3)\n",
    "# fully_conn1=fully_connected(Max_pool_layer3, 128, activation='relu')\n",
    "# fully_conn2=fully_connected(fully_conn1, 32, activation='relu')\n",
    "# fully_conn2_softmax=fully_connected(fully_conn2, 6, activation='softmax')\n",
    "# CNN_model=regression(fully_conn2_softmax,optimizer='SGD', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n",
    "# model=tflearn.DNN(CNN_model, tensorboard_dir='log', tensorboard_verbose=3)\n",
    "# print(X_train.shape)\n",
    "\n",
    "# print(X_train[0].shape)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-24T23:14:19.762221Z",
     "iopub.status.idle": "2022-12-24T23:14:19.763120Z",
     "shell.execute_reply.started": "2022-12-24T23:14:19.762849Z",
     "shell.execute_reply": "2022-12-24T23:14:19.762875Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# AlexNet with Keras\n",
    "# #Instantiate an empty model\n",
    "# image_shape=(IMG_SIZE,IMG_SIZE,3)\n",
    "# model_AlexNet = Sequential()\n",
    "# #first_convoltion_layer\n",
    "# model_AlexNet.add(Conv2D(filters=96,input_shape=image_shape,kernel_size=(11,11),strides=(4,4),padding='valid'))\n",
    "# model_AlexNet.add(Activation('relu'))\n",
    "# # Max Pooling1\n",
    "# model_AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "# #second_convolution_layer\n",
    "# model_AlexNet.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n",
    "# model_AlexNet.add(Activation('relu'))\n",
    "# # Max Pooling2\n",
    "# model_AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "# #third_convolution_layer\n",
    "# model_AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "# model_AlexNet.add(Activation('relu'))\n",
    "# #fourth_convolution_layer\n",
    "# model_AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "# model_AlexNet.add(Activation('relu'))\n",
    "# #fifth_convolution_layer\n",
    "# model_AlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "# model_AlexNet.add(Activation('relu'))\n",
    "# # Max Pooling3\n",
    "# model_AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "# # Passing it to a Fully Connected layer\n",
    "# model_AlexNet.add(Flatten())\n",
    "# #first_fully_connectedLayer\n",
    "# model_AlexNet.add(Dense(4096, input_shape=image_shape))\n",
    "# model_AlexNet.add(Activation('relu'))\n",
    "# # Adding Dropout to prevent overfitting\n",
    "# model_AlexNet.add(Dropout(0.4))\n",
    "# #second_fully_connectedLayer\n",
    "# model_AlexNet.add(Dense(4096))\n",
    "# model_AlexNet.add(Activation('relu'))\n",
    "# # Adding Dropout to prevent overfitting\n",
    "# model_AlexNet.add(Dropout(0.4))\n",
    "# #output_layer\n",
    "# model_AlexNet.add(Dense(6))\n",
    "# model_AlexNet.add(Activation('softmax'))\n",
    "\n",
    "# model_AlexNet.summary()\n",
    "# #compiling and running model\n",
    "# model_AlexNet.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-24T23:14:19.764673Z",
     "iopub.status.idle": "2022-12-24T23:14:19.765391Z",
     "shell.execute_reply.started": "2022-12-24T23:14:19.765122Z",
     "shell.execute_reply": "2022-12-24T23:14:19.765148Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # if (os.path.exists('model.tfl.meta')):\n",
    "# #     model.load('/kaggle/working/model.tfl')\n",
    "# # else:\n",
    "# X_train,X_test,Y_train,Y_test = train_test_split(X_train, Y_train, test_size=0.20, train_size=0.80, shuffle=True)\n",
    "# model.fit({'input': X_train}, {'targets': Y_train}, n_epoch = 25,\n",
    "#           validation_set=({'input': X_test}, {'targets': Y_test}),\n",
    "#       snapshot_step=500, show_metric=True, run_id=MODEL_NAME)\n",
    "# model.save('model.tfl')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-24T23:14:19.766908Z",
     "iopub.status.idle": "2022-12-24T23:14:19.767785Z",
     "shell.execute_reply.started": "2022-12-24T23:14:19.767512Z",
     "shell.execute_reply": "2022-12-24T23:14:19.767538Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}